#############################
## Kickstart Configuration ##
#############################

## Accept the VMware End User License Agreement
vmaccepteula

## Clear paritions on all connected disks and install to the SuperMicro SATADOM
#clearpart --alldrives --overwritevmfs
install --firstdisk --overwritevmfs

## Set the root password
rootpw VMware123!

## Host Network Settings
network --bootproto=dhcp --vlanid=31 --addvmportgroup=1

## Reboot without user interactivity
reboot

##########################################
## Post-Install Configuration (BusyBox) ##
##########################################

%firstboot --interpreter=busybox

## Set hostname and DNS search suffix
esxcli network ip dns search add -d lab.altitudeautomations.com
for i in `esxcli network ip interface ipv4 get | grep vmk0 | awk '{ print $2 }'`; do esxcli system hostname set --fqdn=`nslookup $i | grep name | awk '{ print $4 }'`; done

# Enable and start both Remote ESXi shell (SSH) and local ESXi Shell
vim-cmd hostsvc/enable_ssh
vim-cmd hostsvc/start_ssh
vim-cmd hostsvc/enable_esx_shell
vim-cmd hostsvc/start_esx_shell

# supress ESXi Shell shell warning and disable CEIP
esxcli system settings advanced set -o /UserVars/SuppressShellWarning -i 1
esxcli system settings advanced set -o /UserVars/HostClientCEIPOptIn -i 2

################################
## vSwitch/VMK0 configuration ##
################################

## Unlink vmnic0 and replace with vmnic6 on vSwitch0
#esxcli network vswitch standard uplink remove --uplink-name=vmnic0 --vswitch-name=vSwitch0
#esxcli network vswitch standard uplink add --uplink-name=vmnic6 --vswitch-name=vSwitch0

## Set management VLAN to 31
esxcli network vswitch standard portgroup set -p 'Management Network' --vlan-id 31

## configure mtu + cdp + security on vSwitch0
esxcli network vswitch standard set --mtu 9000 --cdp-status listen --vswitch-name vSwitch0

## Configure vmk0 for jumbo frames
esxcli network ip interface set --interface-name vmk0 --mtu 9000

## Configure 'VM Network' portgroup to use v31 in accordance with VCF expected behavior
esxcli network vswitch standard portgroup set -p 'VM Network' --vlan-id 31

##################################
## Miscellaneous Configurations ##
##################################

## Disable IPv6 for VMkernel interfaces
esxcli system module parameters set -m tcpip4 -p ipv6=0
esxcli network ip set --ipv6-enabled=false

## Reset NTP settings to default, set our NTP servers to match VCF config, enable NTP daemon
esxcli system ntp set -r
esxcli system ntp set -s pool.ntp.org -s north-america.pool.ntp.org
esxcli system ntp set -e yes

## Check if cache/capacity disks are already in use by previous VSAN cluster. If yes, destroy the VSAN disk group so they are free for new deployment. If no, do nothing.
## NOTE: This line assumes only ONE disk group per host for lab purposes. If you intend to use it on an enterprise server with more than 1 disk group, remove the '-m 1' within the grep block.
## NESTED NOTE: Commenting out for now since the nested build only leverages net new virtual hardware.  Should be uncommented for physical builds.
#for i in `esxcli vsan storage list | grep -m 1 'VSAN Disk Group UUID' | awk '{print $5}'`; do esxcli vsan storage diskgroup mount -u $i; esxcli vsan storage remove -u $i; done

## Mark the second and third disks (VSAN Cache and Capacity, respectively) as SSD's
esxcli storage hpp device set -d mpx.vmhba1:C0:T0:L0 -M true
esxcli storage hpp device set -d mpx.vmhba2:C0:T0:L0 -M true

## Set the isCapacityFlash flag to true on the third (VSAN Capacity) disk
esxcli vsan storage tag add -t capacityFlash -d mpx.vmhba2:C0:T0:L0

## Enable the fake SCSI workaround ot prevent VCF from giving you the middle finger due to nested hosts NOT being supported
esxcli system settings advanced set -o /VSAN/FakeSCSIReservations -i 1

## Regenerate self-signed certificate with the new FQDN hostname and restart services before commissioning into VCF
/sbin/generate-certificates
/etc/init.d/hostd restart && /etc/init.d/vpxa restart

### DONE ###